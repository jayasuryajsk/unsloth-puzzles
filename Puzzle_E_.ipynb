{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMs5BmvC4ZYl1IMgkYKpdzx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayasuryajsk/unsloth-puzzles/blob/main/Puzzle_E_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution for Puzzle E: **Memory Efficient Backprop**\n",
        "\n",
        "Used: Cusrosr - R1"
      ],
      "metadata": {
        "id": "KH7WLokyyoaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import functools\n",
        "import gc\n",
        "\n",
        "# Custom bfloat16 cross-entropy loss\n",
        "def bfloat16_cross_entropy(logits, targets, reduction=\"mean\"):\n",
        "    \"\"\"\n",
        "    Compute cross-entropy loss in bfloat16 without upcasting.\n",
        "    Args:\n",
        "        logits: (batch_size, vocab_size) in bfloat16\n",
        "        targets: (batch_size,) integer labels\n",
        "        reduction: 'mean' or 'sum'\n",
        "    Returns:\n",
        "        loss: Scalar loss in bfloat16\n",
        "    \"\"\"\n",
        "    assert logits.dtype == torch.bfloat16, \"Logits must be bfloat16\"\n",
        "\n",
        "    # Subtract max for numerical stability\n",
        "    logits_max, _ = torch.max(logits, dim=-1, keepdim=True)\n",
        "    logits_stable = logits - logits_max\n",
        "\n",
        "    # Softmax: exp(logits) / sum(exp(logits))\n",
        "    exp_logits = torch.exp(logits_stable)\n",
        "    sum_exp_logits = torch.sum(exp_logits, dim=-1, keepdim=True)\n",
        "    log_softmax = logits_stable - torch.log(sum_exp_logits)\n",
        "\n",
        "    # Gather log probabilities for targets\n",
        "    target_log_probs = log_softmax.gather(dim=-1, index=targets.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    # Negative log likelihood\n",
        "    loss = -target_log_probs\n",
        "\n",
        "    # Reduction\n",
        "    if reduction == \"mean\":\n",
        "        return loss.mean()\n",
        "    elif reduction == \"sum\":\n",
        "        return loss.sum()\n",
        "    else:\n",
        "        return loss\n",
        "\n",
        "# Updated transformation function\n",
        "def transformation_function(batch, linear, labels):\n",
        "    x = linear(batch)  # Keep in bfloat16\n",
        "    loss = bfloat16_cross_entropy(\n",
        "        x.view(-1, x.shape[-1]),\n",
        "        labels.view(-1),\n",
        "        reduction=\"mean\"\n",
        "    )\n",
        "    return loss\n",
        "\n",
        "# Memory-efficient linear layer with autograd\n",
        "class MemoryEfficientLinear(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, X, linear, labels, forward_function):\n",
        "        # Save tensors needed for backward pass\n",
        "        ctx.save_for_backward(X)\n",
        "        ctx.linear = linear\n",
        "        ctx.labels = labels\n",
        "        ctx.chunk_size = 1024  # Default chunk size\n",
        "\n",
        "        # Compute output in chunks to save memory\n",
        "        batch_size = X.size(0)\n",
        "        outputs = []\n",
        "\n",
        "        for i in range(0, batch_size, ctx.chunk_size):\n",
        "            chunk = X[i:i+ctx.chunk_size]\n",
        "            chunk_labels = labels[i:i+ctx.chunk_size] if labels is not None else None\n",
        "            chunk_loss = forward_function(chunk, linear, chunk_labels)\n",
        "            outputs.append(chunk_loss)\n",
        "\n",
        "        # Average the losses from all chunks\n",
        "        return sum(outputs) / len(outputs) if len(outputs) > 1 else outputs[0]\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        X, = ctx.saved_tensors\n",
        "        linear = ctx.linear\n",
        "        labels = ctx.labels\n",
        "        chunk_size = ctx.chunk_size\n",
        "\n",
        "        # Initialize gradient accumulators\n",
        "        grad_X = torch.zeros_like(X, dtype=X.dtype)\n",
        "        grad_weight = torch.zeros_like(linear.weight, dtype=linear.weight.dtype)\n",
        "\n",
        "        # Compute gradients in chunks\n",
        "        for i in range(0, X.size(0), chunk_size):\n",
        "            chunk = X[i:i+chunk_size]\n",
        "            chunk_labels = labels[i:i+chunk_size] if labels is not None else None\n",
        "\n",
        "            # Forward pass for this chunk with grad enabled\n",
        "            chunk.requires_grad_(True)\n",
        "            chunk_output = linear(chunk)  # Keep bfloat16\n",
        "            loss = bfloat16_cross_entropy(\n",
        "                chunk_output.view(-1, chunk_output.shape[-1]),\n",
        "                chunk_labels.view(-1) if chunk_labels is not None else None,\n",
        "                reduction=\"mean\"\n",
        "            )\n",
        "\n",
        "            # Backward pass for this chunk\n",
        "            chunk_grad = torch.autograd.grad(\n",
        "                loss, [chunk, linear.weight],\n",
        "                grad_output,\n",
        "                retain_graph=False\n",
        "            )\n",
        "\n",
        "            # Accumulate gradients\n",
        "            grad_X[i:i+chunk_size] = chunk_grad[0]\n",
        "            grad_weight += chunk_grad[1]\n",
        "\n",
        "        return grad_X, grad_weight, None, None\n",
        "\n",
        "def memory_efficient_forward(X, linear, labels):\n",
        "    return MemoryEfficientLinear.apply(X, linear, labels, transformation_function)\n",
        "\n",
        "# Memory-efficient matrix multiplication\n",
        "class MemoryEfficientMatmul(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, X, W, chunk_size):\n",
        "        W = W.to(dtype=X.dtype)\n",
        "        ctx.save_for_backward(X, W)\n",
        "        ctx.chunk_size = chunk_size\n",
        "\n",
        "        output = []\n",
        "        for i in range(0, X.size(0), chunk_size):\n",
        "            output.append(X[i:i+chunk_size] @ W.T)\n",
        "        return torch.cat(output, dim=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        X, W = ctx.saved_tensors\n",
        "        chunk_size = ctx.chunk_size\n",
        "\n",
        "        grad_output = grad_output.to(dtype=X.dtype)\n",
        "        grad_X = torch.zeros_like(X)\n",
        "        grad_W = torch.zeros_like(W)\n",
        "\n",
        "        for i in range(0, X.size(0), chunk_size):\n",
        "            X_chunk = X[i:i+chunk_size]\n",
        "            grad_output_chunk = grad_output[i:i+chunk_size]\n",
        "            grad_X[i:i+chunk_size] = grad_output_chunk @ W\n",
        "            grad_W += grad_output_chunk.T @ X_chunk\n",
        "\n",
        "        return grad_X, grad_W, None\n",
        "\n",
        "def memory_efficient_matmul(X, W, chunk_size=1024):\n",
        "    return MemoryEfficientMatmul.apply(X, W, chunk_size)\n",
        "\n",
        "# Memory-efficient linear module\n",
        "class EfficientLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, chunk_size=1024):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features, dtype=torch.bfloat16))\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "    def forward(self, X, labels=None):\n",
        "        if labels is not None:\n",
        "            return memory_efficient_forward(X, self, labels)\n",
        "        return memory_efficient_matmul(X, self.weight, self.chunk_size)\n",
        "\n",
        "    def adjust_chunk_size(self, new_size):\n",
        "        self.chunk_size = new_size\n",
        "\n",
        "# Naive linear for comparison\n",
        "class NaiveLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
        "\n",
        "    def forward(self, X):\n",
        "        return X @ self.weight.T\n",
        "\n",
        "# Memory measurement decorator\n",
        "def measure_memory(func):\n",
        "    @functools.wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        initial_mem = torch.cuda.max_memory_allocated()\n",
        "\n",
        "        result = func(*args, **kwargs)\n",
        "\n",
        "        peak_mem = torch.cuda.max_memory_allocated() - initial_mem\n",
        "        wrapper.peak_memory = peak_mem\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "# Test functions\n",
        "def test_vram_reduction():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA not available, skipping VRAM test\")\n",
        "        return\n",
        "\n",
        "    bsz, hd, vocab = 4, 4096, 128000\n",
        "    inputs = torch.randn(bsz, hd, dtype=torch.bfloat16, device=device)\n",
        "\n",
        "    @measure_memory\n",
        "    def run_naive():\n",
        "        model = NaiveLinear(hd, vocab).to(device).to(dtype=torch.bfloat16)\n",
        "        out = model(inputs)\n",
        "        return out\n",
        "\n",
        "    @measure_memory\n",
        "    def run_efficient():\n",
        "        model = EfficientLinear(hd, vocab, chunk_size=2).to(device).to(dtype=torch.bfloat16)\n",
        "        out = model(inputs)\n",
        "        return out\n",
        "\n",
        "    _ = run_naive()\n",
        "    naive_mem = run_naive.peak_memory\n",
        "\n",
        "    _ = run_efficient()\n",
        "    efficient_mem = run_efficient.peak_memory\n",
        "\n",
        "    reduction = (naive_mem - efficient_mem) / naive_mem\n",
        "    print(f\"Memory reduction: {reduction*100:.2f}%\")\n",
        "    return reduction >= 0.5\n",
        "\n",
        "def test_float32_upcast():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = EfficientLinear(4096, 128000).to(device).to(dtype=torch.bfloat16)\n",
        "    inputs = torch.randn(4, 4096, dtype=torch.bfloat16, device=device)\n",
        "    labels = torch.randint(0, 128000, (4,), device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(inputs, labels)  # Test loss computation\n",
        "    maintains_bfloat16 = output.dtype == torch.bfloat16\n",
        "    print(f\"Maintains bfloat16: {maintains_bfloat16}\")\n",
        "    return maintains_bfloat16\n",
        "\n",
        "def test_other_functions():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = EfficientLinear(4096, 128000).to(device)\n",
        "    inputs = torch.randn(4, 4096, device=device)\n",
        "\n",
        "    activations = {\n",
        "        'relu': F.relu,\n",
        "        'gelu': F.gelu,\n",
        "        'tanh': torch.tanh,\n",
        "        'sigmoid': torch.sigmoid\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for name, activation in activations.items():\n",
        "        try:\n",
        "            output = activation(model(inputs))\n",
        "            results[name] = True\n",
        "        except:\n",
        "            results[name] = False\n",
        "\n",
        "    print(\"Function compatibility:\", results)\n",
        "    return all(results.values())\n",
        "\n",
        "def test_dynamic_chunk_sizes():\n",
        "    model = EfficientLinear(4096, 128000)\n",
        "    inputs = torch.randn(4, 4096)\n",
        "\n",
        "    chunk_sizes = [1, 2, 4]\n",
        "    results = []\n",
        "\n",
        "    for chunk_size in chunk_sizes:\n",
        "        try:\n",
        "            model.chunk_size = chunk_size\n",
        "            output = model(inputs)\n",
        "            results.append(True)\n",
        "        except:\n",
        "            results.append(False)\n",
        "\n",
        "    success = all(results)\n",
        "    print(f\"Dynamic chunk sizes work: {success}\")\n",
        "    return success\n",
        "\n",
        "\n",
        "def run_all_tests():\n",
        "    score = 0\n",
        "\n",
        "    if test_vram_reduction():\n",
        "        score += 2\n",
        "        print(\"✓ VRAM reduction test passed (+2)\")\n",
        "\n",
        "    if not test_float32_upcast():\n",
        "        score = 0\n",
        "        print(\"✗ Float32 upcast test failed (score reset to 0)\")\n",
        "    else:\n",
        "        print(\"✓ Float32 upcast test passed\")\n",
        "\n",
        "\n",
        "    if test_other_functions():\n",
        "        score += 1\n",
        "        print(\"✓ Other functions test passed (+1)\")\n",
        "\n",
        "    if test_dynamic_chunk_sizes():\n",
        "        score += 1\n",
        "        print(\"✓ Dynamic chunk sizes test passed (+1)\")\n",
        "\n",
        "    print(f\"\\nFinal score: {score}/4\")\n",
        "    return score\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_all_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjbqPGTuvLJr",
        "outputId": "8e71d42e-26ca-4ece-accb-22a3dc25c240"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory reduction: 66.60%\n",
            "✓ VRAM reduction test passed (+2)\n",
            "Maintains bfloat16: True\n",
            "✓ Float32 upcast test passed\n",
            "Function compatibility: {'relu': True, 'gelu': True, 'tanh': True, 'sigmoid': True}\n",
            "✓ Other functions test passed (+1)\n",
            "Dynamic chunk sizes work: True\n",
            "✓ Dynamic chunk sizes test passed (+1)\n",
            "\n",
            "Final score: 4/4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JNb79OjUyRMT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}